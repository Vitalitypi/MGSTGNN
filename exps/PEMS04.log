/mnt/workspace
Namespace(dataset='PEMS04', mode='train', device='cuda:0', debug=False, model='MGSTGNN', cuda=True, val_ratio=0.2, test_ratio=0.2, in_steps=12, out_steps=12, num_nodes=307, normalizer='std', adj_norm=False, input_dim=5, flow_dim=1, period_dim=1, weekend_dim=1, holiday_dim=1, hop_dim=1, weather_dim=0, dim_discriminator=256, alpha_discriminator=0.2, use_discriminator=False, use_embs=False, num_input_dim=1, input_embedding_dim=0, periods_embedding_dim=[10], weekend_embedding_dim=10, holiday_embedding_dim=0, spatial_embedding_dim=0, adaptive_embedding_dim=0, output_dim=1, embed_dim=10, rnn_units=64, num_grus=[1, 2], periods=[288], predict_time=2, use_back=True, loss_func='mae', random=False, seed=10, batch_size=64, epochs=35, lr_init=0.007, lr_decay=True, lr_decay_rate=0.3, lr_decay_step='15,30', early_stop=True, early_stop_patience=15, grad_norm=False, max_grad_norm=5, real_value=True, mae_thresh=None, mape_thresh=0.0, log_dir='./', log_step=20, plot=False)
*****************Model Parameter*****************
mgstgnn.encoder.node_embeddings torch.Size([307, 10]) True
mgstgnn.encoder.time_embeddings torch.Size([64, 12, 10]) True
mgstgnn.encoder.periods_embedding.0.weight torch.Size([288, 10]) True
mgstgnn.encoder.weekend_embedding.weight torch.Size([7, 10]) True
mgstgnn.predictor.grus.0.gate.weights_pool torch.Size([10, 2, 65, 128]) True
mgstgnn.predictor.grus.0.gate.bias_pool torch.Size([10, 128]) True
mgstgnn.predictor.grus.0.gate.norm.weight torch.Size([10]) True
mgstgnn.predictor.grus.0.gate.norm.bias torch.Size([10]) True
mgstgnn.predictor.grus.0.update.weights_pool torch.Size([10, 2, 65, 64]) True
mgstgnn.predictor.grus.0.update.bias_pool torch.Size([10, 64]) True
mgstgnn.predictor.grus.0.update.norm.weight torch.Size([10]) True
mgstgnn.predictor.grus.0.update.norm.bias torch.Size([10]) True
mgstgnn.predictor.grus.1.gate.weights_pool torch.Size([10, 2, 65, 128]) True
mgstgnn.predictor.grus.1.gate.bias_pool torch.Size([10, 128]) True
mgstgnn.predictor.grus.1.gate.norm.weight torch.Size([10]) True
mgstgnn.predictor.grus.1.gate.norm.bias torch.Size([10]) True
mgstgnn.predictor.grus.1.update.weights_pool torch.Size([10, 2, 65, 64]) True
mgstgnn.predictor.grus.1.update.bias_pool torch.Size([10, 64]) True
mgstgnn.predictor.grus.1.update.norm.weight torch.Size([10]) True
mgstgnn.predictor.grus.1.update.norm.bias torch.Size([10]) True
mgstgnn.predictor.grus.2.gate.weights_pool torch.Size([10, 2, 65, 128]) True
mgstgnn.predictor.grus.2.gate.bias_pool torch.Size([10, 128]) True
mgstgnn.predictor.grus.2.gate.norm.weight torch.Size([10]) True
mgstgnn.predictor.grus.2.gate.norm.bias torch.Size([10]) True
mgstgnn.predictor.grus.2.update.weights_pool torch.Size([10, 2, 65, 64]) True
mgstgnn.predictor.grus.2.update.bias_pool torch.Size([10, 64]) True
mgstgnn.predictor.grus.2.update.norm.weight torch.Size([10]) True
mgstgnn.predictor.grus.2.update.norm.bias torch.Size([10]) True
mgstgnn.predictor.backs.0.weight torch.Size([1, 64]) True
mgstgnn.predictor.backs.0.bias torch.Size([1]) True
mgstgnn.predictor.backs.1.weight torch.Size([1, 64]) True
mgstgnn.predictor.backs.1.bias torch.Size([1]) True
mgstgnn.predictor.backs.2.weight torch.Size([1, 64]) True
mgstgnn.predictor.backs.2.bias torch.Size([1]) True
mgstgnn.predictor.predictors.0.weight torch.Size([12, 2, 1, 64]) True
mgstgnn.predictor.predictors.0.bias torch.Size([12]) True
mgstgnn.predictor.predictors.1.weight torch.Size([12, 2, 1, 64]) True
mgstgnn.predictor.predictors.1.bias torch.Size([12]) True
mgstgnn.predictor.skips.0.weight torch.Size([12, 2, 1, 64]) True
mgstgnn.predictor.skips.0.bias torch.Size([12]) True
Total params num: 773219
*****************Finish Parameter****************
(16992, 307, 5)
Train:  (10173, 12, 307, 5) (10173, 12, 307, 1)
Val:  (3375, 12, 307, 5) (3375, 12, 307, 1)
Test:  (3375, 12, 307, 5) (3375, 12, 307, 1)
Applying learning rate decay.
Creat Log File in:  /mnt/workspace/MGSTGNN/exps/logs/PEMS04/20231126115601/run.log
2023-11-26 11:56: Namespace(dataset='PEMS04', mode='train', device='cuda:0', debug=False, model='MGSTGNN', cuda=True, val_ratio=0.2, test_ratio=0.2, in_steps=12, out_steps=12, num_nodes=307, normalizer='std', adj_norm=False, input_dim=5, flow_dim=1, period_dim=1, weekend_dim=1, holiday_dim=1, hop_dim=1, weather_dim=0, dim_discriminator=256, alpha_discriminator=0.2, use_discriminator=False, use_embs=False, num_input_dim=1, input_embedding_dim=0, periods_embedding_dim=[10], weekend_embedding_dim=10, holiday_embedding_dim=0, spatial_embedding_dim=0, adaptive_embedding_dim=0, output_dim=1, embed_dim=10, rnn_units=64, num_grus=[1, 2], periods=[288], predict_time=2, use_back=True, loss_func='mae', random=False, seed=10, batch_size=64, epochs=35, lr_init=0.007, lr_decay=True, lr_decay_rate=0.3, lr_decay_step='15,30', early_stop=True, early_stop_patience=15, grad_norm=False, max_grad_norm=5, real_value=True, mae_thresh=None, mape_thresh=0.0, log_dir='/mnt/workspace/MGSTGNN/exps/logs/PEMS04/20231126115601', log_step=20, plot=False)
2023-11-26 11:56: Experiment log path in: /mnt/workspace/MGSTGNN/exps/logs/PEMS04/20231126115601
2023-11-26 11:56: Train Epoch 1: 20/158 Generator Loss: 43.838009
2023-11-26 11:56: Train Epoch 1: 40/158 Generator Loss: 29.489895
2023-11-26 11:56: Train Epoch 1: 60/158 Generator Loss: 29.567299
2023-11-26 11:56: Train Epoch 1: 80/158 Generator Loss: 27.846397
2023-11-26 11:56: Train Epoch 1: 100/158 Generator Loss: 28.026047
2023-11-26 11:56: Train Epoch 1: 120/158 Generator Loss: 26.022081
2023-11-26 11:56: Train Epoch 1: 140/158 Generator Loss: 25.309761
2023-11-26 11:56: **********Train Epoch 1: Averaged Generator Loss: 39.266581, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 11:56: **********Val Epoch 1: average Loss: 25.950184
2023-11-26 11:56: **********test Epoch 1: average Loss: 24.590604
2023-11-26 11:56: *********************************Current best model saved!
2023-11-26 11:56: Train Epoch 2: 20/158 Generator Loss: 26.326189
2023-11-26 11:56: Train Epoch 2: 40/158 Generator Loss: 25.316551
2023-11-26 11:57: Train Epoch 2: 60/158 Generator Loss: 24.097250
2023-11-26 11:57: Train Epoch 2: 80/158 Generator Loss: 23.496647
2023-11-26 11:57: Train Epoch 2: 100/158 Generator Loss: 24.249239
2023-11-26 11:57: Train Epoch 2: 120/158 Generator Loss: 23.838408
2023-11-26 11:57: Train Epoch 2: 140/158 Generator Loss: 22.257566
2023-11-26 11:57: **********Train Epoch 2: Averaged Generator Loss: 24.198063, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 11:57: **********Val Epoch 2: average Loss: 23.936274
2023-11-26 11:57: **********test Epoch 2: average Loss: 22.819420
2023-11-26 11:57: *********************************Current best model saved!
2023-11-26 11:57: Train Epoch 3: 20/158 Generator Loss: 21.352270
2023-11-26 11:57: Train Epoch 3: 40/158 Generator Loss: 23.636024
2023-11-26 11:57: Train Epoch 3: 60/158 Generator Loss: 23.016636
2023-11-26 11:57: Train Epoch 3: 80/158 Generator Loss: 21.433479
2023-11-26 11:57: Train Epoch 3: 100/158 Generator Loss: 21.991264
2023-11-26 11:57: Train Epoch 3: 120/158 Generator Loss: 21.834703
2023-11-26 11:58: Train Epoch 3: 140/158 Generator Loss: 21.683384
2023-11-26 11:58: **********Train Epoch 3: Averaged Generator Loss: 22.241819, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 11:58: **********Val Epoch 3: average Loss: 23.339305
2023-11-26 11:58: **********test Epoch 3: average Loss: 22.430016
2023-11-26 11:58: *********************************Current best model saved!
2023-11-26 11:58: Train Epoch 4: 20/158 Generator Loss: 21.679129
2023-11-26 11:58: Train Epoch 4: 40/158 Generator Loss: 21.473103
2023-11-26 11:58: Train Epoch 4: 60/158 Generator Loss: 21.457151
2023-11-26 11:58: Train Epoch 4: 80/158 Generator Loss: 20.483900
2023-11-26 11:58: Train Epoch 4: 100/158 Generator Loss: 20.096287
2023-11-26 11:58: Train Epoch 4: 120/158 Generator Loss: 21.156729
2023-11-26 11:58: Train Epoch 4: 140/158 Generator Loss: 22.124157
2023-11-26 11:58: **********Train Epoch 4: Averaged Generator Loss: 21.545155, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 11:58: **********Val Epoch 4: average Loss: 21.855186
2023-11-26 11:58: **********test Epoch 4: average Loss: 21.007026
2023-11-26 11:58: *********************************Current best model saved!
2023-11-26 11:59: Train Epoch 5: 20/158 Generator Loss: 19.580841
2023-11-26 11:59: Train Epoch 5: 40/158 Generator Loss: 20.005932
2023-11-26 11:59: Train Epoch 5: 60/158 Generator Loss: 20.896296
2023-11-26 11:59: Train Epoch 5: 80/158 Generator Loss: 20.694910
2023-11-26 11:59: Train Epoch 5: 100/158 Generator Loss: 21.418373
2023-11-26 11:59: Train Epoch 5: 120/158 Generator Loss: 20.870836
2023-11-26 11:59: Train Epoch 5: 140/158 Generator Loss: 20.127893
2023-11-26 11:59: **********Train Epoch 5: Averaged Generator Loss: 20.664304, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 11:59: **********Val Epoch 5: average Loss: 21.269158
2023-11-26 11:59: **********test Epoch 5: average Loss: 20.533735
2023-11-26 11:59: *********************************Current best model saved!
2023-11-26 11:59: Train Epoch 6: 20/158 Generator Loss: 20.772051
2023-11-26 11:59: Train Epoch 6: 40/158 Generator Loss: 19.467554
2023-11-26 11:59: Train Epoch 6: 60/158 Generator Loss: 21.997572
2023-11-26 11:59: Train Epoch 6: 80/158 Generator Loss: 19.173967
2023-11-26 12:00: Train Epoch 6: 100/158 Generator Loss: 20.020473
2023-11-26 12:00: Train Epoch 6: 120/158 Generator Loss: 20.533991
2023-11-26 12:00: Train Epoch 6: 140/158 Generator Loss: 20.561342
2023-11-26 12:00: **********Train Epoch 6: Averaged Generator Loss: 20.209399, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:00: **********Val Epoch 6: average Loss: 20.921958
2023-11-26 12:00: **********test Epoch 6: average Loss: 20.333428
2023-11-26 12:00: *********************************Current best model saved!
2023-11-26 12:00: Train Epoch 7: 20/158 Generator Loss: 21.016071
2023-11-26 12:00: Train Epoch 7: 40/158 Generator Loss: 18.295023
2023-11-26 12:00: Train Epoch 7: 60/158 Generator Loss: 20.946859
2023-11-26 12:00: Train Epoch 7: 80/158 Generator Loss: 19.774515
2023-11-26 12:00: Train Epoch 7: 100/158 Generator Loss: 19.032415
2023-11-26 12:00: Train Epoch 7: 120/158 Generator Loss: 19.657040
2023-11-26 12:00: Train Epoch 7: 140/158 Generator Loss: 21.286085
2023-11-26 12:01: **********Train Epoch 7: Averaged Generator Loss: 19.872300, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:01: **********Val Epoch 7: average Loss: 20.988921
2023-11-26 12:01: **********test Epoch 7: average Loss: 20.472904
2023-11-26 12:01: Train Epoch 8: 20/158 Generator Loss: 21.664869
2023-11-26 12:01: Train Epoch 8: 40/158 Generator Loss: 17.948133
2023-11-26 12:01: Train Epoch 8: 60/158 Generator Loss: 19.075851
2023-11-26 12:01: Train Epoch 8: 80/158 Generator Loss: 19.907869
2023-11-26 12:01: Train Epoch 8: 100/158 Generator Loss: 20.338665
2023-11-26 12:01: Train Epoch 8: 120/158 Generator Loss: 21.003792
2023-11-26 12:01: Train Epoch 8: 140/158 Generator Loss: 19.843653
2023-11-26 12:01: **********Train Epoch 8: Averaged Generator Loss: 19.652189, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:01: **********Val Epoch 8: average Loss: 20.330531
2023-11-26 12:01: **********test Epoch 8: average Loss: 19.875918
2023-11-26 12:01: *********************************Current best model saved!
2023-11-26 12:01: Train Epoch 9: 20/158 Generator Loss: 20.947802
2023-11-26 12:01: Train Epoch 9: 40/158 Generator Loss: 19.137186
2023-11-26 12:02: Train Epoch 9: 60/158 Generator Loss: 19.605394
2023-11-26 12:02: Train Epoch 9: 80/158 Generator Loss: 18.194019
2023-11-26 12:02: Train Epoch 9: 100/158 Generator Loss: 18.767910
2023-11-26 12:02: Train Epoch 9: 120/158 Generator Loss: 19.809435
2023-11-26 12:02: Train Epoch 9: 140/158 Generator Loss: 18.688501
2023-11-26 12:02: **********Train Epoch 9: Averaged Generator Loss: 19.343674, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:02: **********Val Epoch 9: average Loss: 19.840709
2023-11-26 12:02: **********test Epoch 9: average Loss: 19.429343
2023-11-26 12:02: *********************************Current best model saved!
2023-11-26 12:02: Train Epoch 10: 20/158 Generator Loss: 19.313314
2023-11-26 12:02: Train Epoch 10: 40/158 Generator Loss: 18.279346
2023-11-26 12:02: Train Epoch 10: 60/158 Generator Loss: 20.780249
2023-11-26 12:02: Train Epoch 10: 80/158 Generator Loss: 20.043121
2023-11-26 12:02: Train Epoch 10: 100/158 Generator Loss: 20.181604
2023-11-26 12:03: Train Epoch 10: 120/158 Generator Loss: 19.878223
2023-11-26 12:03: Train Epoch 10: 140/158 Generator Loss: 18.838131
2023-11-26 12:03: **********Train Epoch 10: Averaged Generator Loss: 19.068527, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:03: **********Val Epoch 10: average Loss: 19.826908
2023-11-26 12:03: **********test Epoch 10: average Loss: 19.411059
2023-11-26 12:03: *********************************Current best model saved!
2023-11-26 12:03: Train Epoch 11: 20/158 Generator Loss: 19.414967
2023-11-26 12:03: Train Epoch 11: 40/158 Generator Loss: 19.484688
2023-11-26 12:03: Train Epoch 11: 60/158 Generator Loss: 17.631887
2023-11-26 12:03: Train Epoch 11: 80/158 Generator Loss: 19.888697
2023-11-26 12:03: Train Epoch 11: 100/158 Generator Loss: 19.314981
2023-11-26 12:03: Train Epoch 11: 120/158 Generator Loss: 16.966566
2023-11-26 12:03: Train Epoch 11: 140/158 Generator Loss: 19.097815
2023-11-26 12:03: **********Train Epoch 11: Averaged Generator Loss: 18.846154, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:03: **********Val Epoch 11: average Loss: 19.544224
2023-11-26 12:03: **********test Epoch 11: average Loss: 19.253734
2023-11-26 12:03: *********************************Current best model saved!
2023-11-26 12:04: Train Epoch 12: 20/158 Generator Loss: 19.473129
2023-11-26 12:04: Train Epoch 12: 40/158 Generator Loss: 18.720556
2023-11-26 12:04: Train Epoch 12: 60/158 Generator Loss: 20.831800
2023-11-26 12:04: Train Epoch 12: 80/158 Generator Loss: 17.635208
2023-11-26 12:04: Train Epoch 12: 100/158 Generator Loss: 18.395924
2023-11-26 12:04: Train Epoch 12: 120/158 Generator Loss: 17.974434
2023-11-26 12:04: Train Epoch 12: 140/158 Generator Loss: 17.110563
2023-11-26 12:04: **********Train Epoch 12: Averaged Generator Loss: 18.751388, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:04: **********Val Epoch 12: average Loss: 19.545862
2023-11-26 12:04: **********test Epoch 12: average Loss: 19.161589
2023-11-26 12:04: Train Epoch 13: 20/158 Generator Loss: 17.485620
2023-11-26 12:04: Train Epoch 13: 40/158 Generator Loss: 19.373257
2023-11-26 12:04: Train Epoch 13: 60/158 Generator Loss: 18.831318
2023-11-26 12:05: Train Epoch 13: 80/158 Generator Loss: 18.494987
2023-11-26 12:05: Train Epoch 13: 100/158 Generator Loss: 19.941177
2023-11-26 12:05: Train Epoch 13: 120/158 Generator Loss: 18.184591
2023-11-26 12:05: Train Epoch 13: 140/158 Generator Loss: 18.017488
2023-11-26 12:05: **********Train Epoch 13: Averaged Generator Loss: 18.445270, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:05: **********Val Epoch 13: average Loss: 19.264968
2023-11-26 12:05: **********test Epoch 13: average Loss: 18.942752
2023-11-26 12:05: *********************************Current best model saved!
2023-11-26 12:05: Train Epoch 14: 20/158 Generator Loss: 16.981176
2023-11-26 12:05: Train Epoch 14: 40/158 Generator Loss: 18.225780
2023-11-26 12:05: Train Epoch 14: 60/158 Generator Loss: 18.932161
2023-11-26 12:05: Train Epoch 14: 80/158 Generator Loss: 17.179499
2023-11-26 12:05: Train Epoch 14: 100/158 Generator Loss: 17.645975
2023-11-26 12:05: Train Epoch 14: 120/158 Generator Loss: 18.829491
2023-11-26 12:06: Train Epoch 14: 140/158 Generator Loss: 17.795679
2023-11-26 12:06: **********Train Epoch 14: Averaged Generator Loss: 18.440321, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:06: **********Val Epoch 14: average Loss: 19.197047
2023-11-26 12:06: **********test Epoch 14: average Loss: 18.838136
2023-11-26 12:06: *********************************Current best model saved!
2023-11-26 12:06: Train Epoch 15: 20/158 Generator Loss: 19.137957
2023-11-26 12:06: Train Epoch 15: 40/158 Generator Loss: 18.824633
2023-11-26 12:06: Train Epoch 15: 60/158 Generator Loss: 18.041048
2023-11-26 12:06: Train Epoch 15: 80/158 Generator Loss: 18.683264
2023-11-26 12:06: Train Epoch 15: 100/158 Generator Loss: 17.632214
2023-11-26 12:06: Train Epoch 15: 120/158 Generator Loss: 17.283020
2023-11-26 12:06: Train Epoch 15: 140/158 Generator Loss: 16.113302
2023-11-26 12:06: **********Train Epoch 15: Averaged Generator Loss: 18.334877, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:06: **********Val Epoch 15: average Loss: 19.085827
2023-11-26 12:06: **********test Epoch 15: average Loss: 18.807167
2023-11-26 12:06: *********************************Current best model saved!
2023-11-26 12:06: Train Epoch 16: 20/158 Generator Loss: 18.938189
2023-11-26 12:07: Train Epoch 16: 40/158 Generator Loss: 18.692165
2023-11-26 12:07: Train Epoch 16: 60/158 Generator Loss: 17.111670
2023-11-26 12:07: Train Epoch 16: 80/158 Generator Loss: 18.409384
2023-11-26 12:07: Train Epoch 16: 100/158 Generator Loss: 16.637129
2023-11-26 12:07: Train Epoch 16: 120/158 Generator Loss: 20.188219
2023-11-26 12:07: Train Epoch 16: 140/158 Generator Loss: 19.364050
2023-11-26 12:07: **********Train Epoch 16: Averaged Generator Loss: 17.884074, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:07: **********Val Epoch 16: average Loss: 18.811999
2023-11-26 12:07: **********test Epoch 16: average Loss: 18.575767
2023-11-26 12:07: *********************************Current best model saved!
2023-11-26 12:07: Train Epoch 17: 20/158 Generator Loss: 19.304411
2023-11-26 12:07: Train Epoch 17: 40/158 Generator Loss: 18.463739
2023-11-26 12:07: Train Epoch 17: 60/158 Generator Loss: 17.846689
2023-11-26 12:07: Train Epoch 17: 80/158 Generator Loss: 18.903770
2023-11-26 12:08: Train Epoch 17: 100/158 Generator Loss: 16.295738
2023-11-26 12:08: Train Epoch 17: 120/158 Generator Loss: 16.666262
2023-11-26 12:08: Train Epoch 17: 140/158 Generator Loss: 18.215942
2023-11-26 12:08: **********Train Epoch 17: Averaged Generator Loss: 17.792807, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:08: **********Val Epoch 17: average Loss: 18.791554
2023-11-26 12:08: **********test Epoch 17: average Loss: 18.552651
2023-11-26 12:08: *********************************Current best model saved!
2023-11-26 12:08: Train Epoch 18: 20/158 Generator Loss: 17.143431
2023-11-26 12:08: Train Epoch 18: 40/158 Generator Loss: 17.742558
2023-11-26 12:08: Train Epoch 18: 60/158 Generator Loss: 18.507416
2023-11-26 12:08: Train Epoch 18: 80/158 Generator Loss: 19.078022
2023-11-26 12:08: Train Epoch 18: 100/158 Generator Loss: 15.898059
2023-11-26 12:08: Train Epoch 18: 120/158 Generator Loss: 19.062937
2023-11-26 12:08: Train Epoch 18: 140/158 Generator Loss: 17.645281
2023-11-26 12:08: **********Train Epoch 18: Averaged Generator Loss: 17.739554, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:09: **********Val Epoch 18: average Loss: 18.756271
2023-11-26 12:09: **********test Epoch 18: average Loss: 18.515376
2023-11-26 12:09: *********************************Current best model saved!
2023-11-26 12:09: Train Epoch 19: 20/158 Generator Loss: 16.294968
2023-11-26 12:09: Train Epoch 19: 40/158 Generator Loss: 18.098614
2023-11-26 12:09: Train Epoch 19: 60/158 Generator Loss: 17.601208
2023-11-26 12:09: Train Epoch 19: 80/158 Generator Loss: 17.558695
2023-11-26 12:09: Train Epoch 19: 100/158 Generator Loss: 18.118586
2023-11-26 12:09: Train Epoch 19: 120/158 Generator Loss: 18.439631
2023-11-26 12:09: Train Epoch 19: 140/158 Generator Loss: 16.922800
2023-11-26 12:09: **********Train Epoch 19: Averaged Generator Loss: 17.693639, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:09: **********Val Epoch 19: average Loss: 18.748095
2023-11-26 12:09: **********test Epoch 19: average Loss: 18.490978
2023-11-26 12:09: *********************************Current best model saved!
2023-11-26 12:09: Train Epoch 20: 20/158 Generator Loss: 17.367964
2023-11-26 12:09: Train Epoch 20: 40/158 Generator Loss: 17.062531
2023-11-26 12:10: Train Epoch 20: 60/158 Generator Loss: 16.419638
2023-11-26 12:10: Train Epoch 20: 80/158 Generator Loss: 17.829729
2023-11-26 12:10: Train Epoch 20: 100/158 Generator Loss: 17.806347
2023-11-26 12:10: Train Epoch 20: 120/158 Generator Loss: 18.431448
2023-11-26 12:10: Train Epoch 20: 140/158 Generator Loss: 16.784714
2023-11-26 12:10: **********Train Epoch 20: Averaged Generator Loss: 17.662563, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:10: **********Val Epoch 20: average Loss: 18.706108
2023-11-26 12:10: **********test Epoch 20: average Loss: 18.450426
2023-11-26 12:10: *********************************Current best model saved!
2023-11-26 12:10: Train Epoch 21: 20/158 Generator Loss: 16.751869
2023-11-26 12:10: Train Epoch 21: 40/158 Generator Loss: 16.958307
2023-11-26 12:10: Train Epoch 21: 60/158 Generator Loss: 17.902557
2023-11-26 12:10: Train Epoch 21: 80/158 Generator Loss: 17.371883
2023-11-26 12:10: Train Epoch 21: 100/158 Generator Loss: 17.738213
2023-11-26 12:10: Train Epoch 21: 120/158 Generator Loss: 17.854721
2023-11-26 12:11: Train Epoch 21: 140/158 Generator Loss: 17.679567
2023-11-26 12:11: **********Train Epoch 21: Averaged Generator Loss: 17.590883, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:11: **********Val Epoch 21: average Loss: 18.722972
2023-11-26 12:11: **********test Epoch 21: average Loss: 18.470521
2023-11-26 12:11: Train Epoch 22: 20/158 Generator Loss: 18.905300
2023-11-26 12:11: Train Epoch 22: 40/158 Generator Loss: 17.823948
2023-11-26 12:11: Train Epoch 22: 60/158 Generator Loss: 18.048635
2023-11-26 12:11: Train Epoch 22: 80/158 Generator Loss: 16.341179
2023-11-26 12:11: Train Epoch 22: 100/158 Generator Loss: 15.638765
2023-11-26 12:11: Train Epoch 22: 120/158 Generator Loss: 18.024088
2023-11-26 12:11: Train Epoch 22: 140/158 Generator Loss: 17.602066
2023-11-26 12:11: **********Train Epoch 22: Averaged Generator Loss: 17.530068, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:11: **********Val Epoch 22: average Loss: 18.689348
2023-11-26 12:11: **********test Epoch 22: average Loss: 18.442785
2023-11-26 12:11: *********************************Current best model saved!
2023-11-26 12:12: Train Epoch 23: 20/158 Generator Loss: 18.515188
2023-11-26 12:12: Train Epoch 23: 40/158 Generator Loss: 17.711203
2023-11-26 12:12: Train Epoch 23: 60/158 Generator Loss: 18.417509
2023-11-26 12:12: Train Epoch 23: 80/158 Generator Loss: 17.829933
2023-11-26 12:12: Train Epoch 23: 100/158 Generator Loss: 18.254578
2023-11-26 12:12: Train Epoch 23: 120/158 Generator Loss: 17.341528
2023-11-26 12:12: Train Epoch 23: 140/158 Generator Loss: 17.888336
2023-11-26 12:12: **********Train Epoch 23: Averaged Generator Loss: 17.504972, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:12: **********Val Epoch 23: average Loss: 18.671710
2023-11-26 12:12: **********test Epoch 23: average Loss: 18.392631
2023-11-26 12:12: *********************************Current best model saved!
2023-11-26 12:12: Train Epoch 24: 20/158 Generator Loss: 17.470350
2023-11-26 12:12: Train Epoch 24: 40/158 Generator Loss: 17.477674
2023-11-26 12:12: Train Epoch 24: 60/158 Generator Loss: 18.626320
2023-11-26 12:12: Train Epoch 24: 80/158 Generator Loss: 14.570801
2023-11-26 12:13: Train Epoch 24: 100/158 Generator Loss: 18.911081
2023-11-26 12:13: Train Epoch 24: 120/158 Generator Loss: 16.794731
2023-11-26 12:13: Train Epoch 24: 140/158 Generator Loss: 17.584255
2023-11-26 12:13: **********Train Epoch 24: Averaged Generator Loss: 17.458414, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:13: **********Val Epoch 24: average Loss: 18.622467
2023-11-26 12:13: **********test Epoch 24: average Loss: 18.371823
2023-11-26 12:13: *********************************Current best model saved!
2023-11-26 12:13: Train Epoch 25: 20/158 Generator Loss: 17.228495
2023-11-26 12:13: Train Epoch 25: 40/158 Generator Loss: 16.949024
2023-11-26 12:13: Train Epoch 25: 60/158 Generator Loss: 17.006042
2023-11-26 12:13: Train Epoch 25: 80/158 Generator Loss: 17.612814
2023-11-26 12:13: Train Epoch 25: 100/158 Generator Loss: 16.569626
2023-11-26 12:13: Train Epoch 25: 120/158 Generator Loss: 18.248262
2023-11-26 12:13: Train Epoch 25: 140/158 Generator Loss: 17.658304
2023-11-26 12:13: **********Train Epoch 25: Averaged Generator Loss: 17.407534, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:14: **********Val Epoch 25: average Loss: 18.621575
2023-11-26 12:14: **********test Epoch 25: average Loss: 18.370154
2023-11-26 12:14: *********************************Current best model saved!
2023-11-26 12:14: Train Epoch 26: 20/158 Generator Loss: 17.653265
2023-11-26 12:14: Train Epoch 26: 40/158 Generator Loss: 17.581654
2023-11-26 12:14: Train Epoch 26: 60/158 Generator Loss: 17.794365
2023-11-26 12:14: Train Epoch 26: 80/158 Generator Loss: 16.888475
2023-11-26 12:14: Train Epoch 26: 100/158 Generator Loss: 17.829254
2023-11-26 12:14: Train Epoch 26: 120/158 Generator Loss: 16.812737
2023-11-26 12:14: Train Epoch 26: 140/158 Generator Loss: 16.984329
2023-11-26 12:14: **********Train Epoch 26: Averaged Generator Loss: 17.366594, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:14: **********Val Epoch 26: average Loss: 18.607808
2023-11-26 12:14: **********test Epoch 26: average Loss: 18.371420
2023-11-26 12:14: *********************************Current best model saved!
2023-11-26 12:14: Train Epoch 27: 20/158 Generator Loss: 17.014996
2023-11-26 12:14: Train Epoch 27: 40/158 Generator Loss: 17.518930
2023-11-26 12:15: Train Epoch 27: 60/158 Generator Loss: 19.336853
2023-11-26 12:15: Train Epoch 27: 80/158 Generator Loss: 17.936945
2023-11-26 12:15: Train Epoch 27: 100/158 Generator Loss: 17.245972
2023-11-26 12:15: Train Epoch 27: 120/158 Generator Loss: 16.551928
2023-11-26 12:15: Train Epoch 27: 140/158 Generator Loss: 17.717974
2023-11-26 12:15: **********Train Epoch 27: Averaged Generator Loss: 17.318472, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:15: **********Val Epoch 27: average Loss: 18.578072
2023-11-26 12:15: **********test Epoch 27: average Loss: 18.354183
2023-11-26 12:15: *********************************Current best model saved!
2023-11-26 12:15: Train Epoch 28: 20/158 Generator Loss: 18.386765
2023-11-26 12:15: Train Epoch 28: 40/158 Generator Loss: 16.864408
2023-11-26 12:15: Train Epoch 28: 60/158 Generator Loss: 17.818876
2023-11-26 12:15: Train Epoch 28: 80/158 Generator Loss: 18.008072
2023-11-26 12:15: Train Epoch 28: 100/158 Generator Loss: 19.061584
2023-11-26 12:15: Train Epoch 28: 120/158 Generator Loss: 17.102417
2023-11-26 12:16: Train Epoch 28: 140/158 Generator Loss: 16.166689
2023-11-26 12:16: **********Train Epoch 28: Averaged Generator Loss: 17.272206, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:16: **********Val Epoch 28: average Loss: 18.560263
2023-11-26 12:16: **********test Epoch 28: average Loss: 18.325304
2023-11-26 12:16: *********************************Current best model saved!
2023-11-26 12:16: Train Epoch 29: 20/158 Generator Loss: 18.344995
2023-11-26 12:16: Train Epoch 29: 40/158 Generator Loss: 15.915866
2023-11-26 12:16: Train Epoch 29: 60/158 Generator Loss: 17.208544
2023-11-26 12:16: Train Epoch 29: 80/158 Generator Loss: 18.151833
2023-11-26 12:16: Train Epoch 29: 100/158 Generator Loss: 16.847494
2023-11-26 12:16: Train Epoch 29: 120/158 Generator Loss: 18.575235
2023-11-26 12:16: Train Epoch 29: 140/158 Generator Loss: 17.510120
2023-11-26 12:16: **********Train Epoch 29: Averaged Generator Loss: 17.214770, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:16: **********Val Epoch 29: average Loss: 18.587760
2023-11-26 12:16: **********test Epoch 29: average Loss: 18.380833
2023-11-26 12:17: Train Epoch 30: 20/158 Generator Loss: 16.689651
2023-11-26 12:17: Train Epoch 30: 40/158 Generator Loss: 16.253315
2023-11-26 12:17: Train Epoch 30: 60/158 Generator Loss: 16.412415
2023-11-26 12:17: Train Epoch 30: 80/158 Generator Loss: 17.032904
2023-11-26 12:17: Train Epoch 30: 100/158 Generator Loss: 17.557892
2023-11-26 12:17: Train Epoch 30: 120/158 Generator Loss: 17.715090
2023-11-26 12:17: Train Epoch 30: 140/158 Generator Loss: 17.040442
2023-11-26 12:17: **********Train Epoch 30: Averaged Generator Loss: 17.187276, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:17: **********Val Epoch 30: average Loss: 18.522633
2023-11-26 12:17: **********test Epoch 30: average Loss: 18.294612
2023-11-26 12:17: *********************************Current best model saved!
2023-11-26 12:17: Train Epoch 31: 20/158 Generator Loss: 17.670927
2023-11-26 12:17: Train Epoch 31: 40/158 Generator Loss: 17.484844
2023-11-26 12:17: Train Epoch 31: 60/158 Generator Loss: 17.556475
2023-11-26 12:18: Train Epoch 31: 80/158 Generator Loss: 16.104338
2023-11-26 12:18: Train Epoch 31: 100/158 Generator Loss: 16.434052
2023-11-26 12:18: Train Epoch 31: 120/158 Generator Loss: 16.399933
2023-11-26 12:18: Train Epoch 31: 140/158 Generator Loss: 16.080099
2023-11-26 12:18: **********Train Epoch 31: Averaged Generator Loss: 17.046151, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:18: **********Val Epoch 31: average Loss: 18.440077
2023-11-26 12:18: **********test Epoch 31: average Loss: 18.230295
2023-11-26 12:18: *********************************Current best model saved!
2023-11-26 12:18: Train Epoch 32: 20/158 Generator Loss: 15.459970
2023-11-26 12:18: Train Epoch 32: 40/158 Generator Loss: 16.851439
2023-11-26 12:18: Train Epoch 32: 60/158 Generator Loss: 17.219202
2023-11-26 12:18: Train Epoch 32: 80/158 Generator Loss: 17.506973
2023-11-26 12:18: Train Epoch 32: 100/158 Generator Loss: 15.758232
2023-11-26 12:18: Train Epoch 32: 120/158 Generator Loss: 16.812298
2023-11-26 12:18: Train Epoch 32: 140/158 Generator Loss: 19.349735
2023-11-26 12:19: **********Train Epoch 32: Averaged Generator Loss: 17.017533, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:19: **********Val Epoch 32: average Loss: 18.435289
2023-11-26 12:19: **********test Epoch 32: average Loss: 18.240196
2023-11-26 12:19: *********************************Current best model saved!
2023-11-26 12:19: Train Epoch 33: 20/158 Generator Loss: 16.091013
2023-11-26 12:19: Train Epoch 33: 40/158 Generator Loss: 16.932539
2023-11-26 12:19: Train Epoch 33: 60/158 Generator Loss: 15.797450
2023-11-26 12:19: Train Epoch 33: 80/158 Generator Loss: 17.034470
2023-11-26 12:19: Train Epoch 33: 100/158 Generator Loss: 18.315390
2023-11-26 12:19: Train Epoch 33: 120/158 Generator Loss: 17.225275
2023-11-26 12:19: Train Epoch 33: 140/158 Generator Loss: 15.894817
2023-11-26 12:19: **********Train Epoch 33: Averaged Generator Loss: 16.990948, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:19: **********Val Epoch 33: average Loss: 18.419236
2023-11-26 12:19: **********test Epoch 33: average Loss: 18.215251
2023-11-26 12:19: *********************************Current best model saved!
2023-11-26 12:19: Train Epoch 34: 20/158 Generator Loss: 17.137156
2023-11-26 12:20: Train Epoch 34: 40/158 Generator Loss: 17.217119
2023-11-26 12:20: Train Epoch 34: 60/158 Generator Loss: 17.426245
2023-11-26 12:20: Train Epoch 34: 80/158 Generator Loss: 15.632419
2023-11-26 12:20: Train Epoch 34: 100/158 Generator Loss: 16.482117
2023-11-26 12:20: Train Epoch 34: 120/158 Generator Loss: 17.191557
2023-11-26 12:20: Train Epoch 34: 140/158 Generator Loss: 17.328358
2023-11-26 12:20: **********Train Epoch 34: Averaged Generator Loss: 16.972116, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:20: **********Val Epoch 34: average Loss: 18.434293
2023-11-26 12:20: **********test Epoch 34: average Loss: 18.241400
2023-11-26 12:20: Train Epoch 35: 20/158 Generator Loss: 16.890640
2023-11-26 12:20: Train Epoch 35: 40/158 Generator Loss: 17.679232
2023-11-26 12:20: Train Epoch 35: 60/158 Generator Loss: 17.095871
2023-11-26 12:20: Train Epoch 35: 80/158 Generator Loss: 18.152409
2023-11-26 12:21: Train Epoch 35: 100/158 Generator Loss: 17.491030
2023-11-26 12:21: Train Epoch 35: 120/158 Generator Loss: 16.494869
2023-11-26 12:21: Train Epoch 35: 140/158 Generator Loss: 16.193575
2023-11-26 12:21: **********Train Epoch 35: Averaged Generator Loss: 16.972513, Averaged spatial Discriminator Loss: 0.000000, Averaged temporal Discriminator Loss: 0.000000
2023-11-26 12:21: **********Val Epoch 35: average Loss: 18.440081
2023-11-26 12:21: **********test Epoch 35: average Loss: 18.236535
2023-11-26 12:21: Total training time: 25.3201min, best loss: 18.419236
2023-11-26 12:21: Saving current best model to /mnt/workspace/MGSTGNN/exps/logs/PEMS04/20231126115601/best_model.pth
2023-11-26 12:21: Saving current best model to /mnt/workspace/MGSTGNN/exps/logs/PEMS04/20231126115601/best_test_model.pth
2023-11-26 12:21: Horizon 01, MAE: 16.6485, RMSE: 27.2824, MAPE: 11.1359%
2023-11-26 12:21: Horizon 02, MAE: 17.1053, RMSE: 28.2346, MAPE: 11.4399%
2023-11-26 12:21: Horizon 03, MAE: 17.5148, RMSE: 29.0301, MAPE: 11.7102%
2023-11-26 12:21: Horizon 04, MAE: 17.8067, RMSE: 29.6066, MAPE: 11.8384%
2023-11-26 12:21: Horizon 05, MAE: 18.0482, RMSE: 30.1083, MAPE: 12.0098%
2023-11-26 12:21: Horizon 06, MAE: 18.2500, RMSE: 30.5072, MAPE: 12.1495%
2023-11-26 12:21: Horizon 07, MAE: 18.4377, RMSE: 30.8634, MAPE: 12.2620%
2023-11-26 12:21: Horizon 08, MAE: 18.6149, RMSE: 31.1490, MAPE: 12.3589%
2023-11-26 12:21: Horizon 09, MAE: 18.7862, RMSE: 31.4271, MAPE: 12.4846%
2023-11-26 12:21: Horizon 10, MAE: 18.9683, RMSE: 31.7113, MAPE: 12.5351%
2023-11-26 12:21: Horizon 11, MAE: 19.1509, RMSE: 31.9954, MAPE: 12.6823%
2023-11-26 12:21: Horizon 12, MAE: 19.4183, RMSE: 32.3682, MAPE: 12.8743%
2023-11-26 12:21: Average Horizon, MAE: 18.2291, RMSE: 30.3939, MAPE: 12.1234%
2023-11-26 12:21: This is best_test_model
2023-11-26 12:21: Horizon 01, MAE: 16.6485, RMSE: 27.2824, MAPE: 11.1359%
2023-11-26 12:21: Horizon 02, MAE: 17.1053, RMSE: 28.2346, MAPE: 11.4399%
2023-11-26 12:21: Horizon 03, MAE: 17.5148, RMSE: 29.0301, MAPE: 11.7102%
2023-11-26 12:21: Horizon 04, MAE: 17.8067, RMSE: 29.6066, MAPE: 11.8384%
2023-11-26 12:21: Horizon 05, MAE: 18.0482, RMSE: 30.1083, MAPE: 12.0098%
2023-11-26 12:21: Horizon 06, MAE: 18.2500, RMSE: 30.5072, MAPE: 12.1495%
2023-11-26 12:21: Horizon 07, MAE: 18.4377, RMSE: 30.8634, MAPE: 12.2620%
2023-11-26 12:21: Horizon 08, MAE: 18.6149, RMSE: 31.1490, MAPE: 12.3589%
2023-11-26 12:21: Horizon 09, MAE: 18.7862, RMSE: 31.4271, MAPE: 12.4846%
2023-11-26 12:21: Horizon 10, MAE: 18.9683, RMSE: 31.7113, MAPE: 12.5351%
2023-11-26 12:21: Horizon 11, MAE: 19.1509, RMSE: 31.9954, MAPE: 12.6823%
2023-11-26 12:21: Horizon 12, MAE: 19.4183, RMSE: 32.3682, MAPE: 12.8743%
2023-11-26 12:21: Average Horizon, MAE: 18.2291, RMSE: 30.3939, MAPE: 12.1234%
